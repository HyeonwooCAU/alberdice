# Single-agent OptiDICE
buffer_size: 5000

# use epsilon greedy action selector
action_selector:  "multinomial"
epsilon_start: 0.0
epsilon_finish: 0.0
epsilon_anneal_time: 50000
mask_before_softmax: True


lr: 0.0001 # policy default
nu_lr: 0.0001
alpha_start: 5  # balancing term for kl-regularization
alpha_end: 0.001
agent_output_type: "pi_logits"
learner: "optidice_learner"

absorbing_state: True

#env_args:
#  state_last_action: False

name: "optidice"